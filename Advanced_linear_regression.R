# Analyzing the Auto data set with Mulitple Linear Regression
# The first thing is about applying multiple linear regression to analyzing the Auto dataset.
# Explore the correlation between variables,
# use lm() to build a multiple linear regression with mpg as the response variable. 
# Building the regression model is simple. Learn to interpret the output of the lm() function using the summary() function.
# use the plot() function to produce diagnostic plots, e.g., residual plot and QQ plot.
# explore possible interaction between variables
# study how different variable transformations will affect the model.

library(ISLR)
Auto = read.csv("Auto.csv", header = T, na.strings = "?")
Auto = na.omit(Auto) # if there are missing values.
head(Auto)

# Assume that we are not interested in name. Here, name is actually a categorical variable. 
# In the following task, we will not use it, so here we delete it from the dataframe.
# Drop the name column:
Auto$name = NULL

# check the structure of Auto dataframe using str() function:
str(Auto)

# Produce a scatterplot matrix which includes all of the variables in the data set
# Use pairs() function to generate pairwise scatterplots to explore the relationship between each variables.
pairs(Auto)

# Compute the matrix of correlations between the variables using the function cor().
cor(Auto)

library(lattice)
levelplot(cor(Auto), scales = list(x = list(rot = 90)))
# The plot shows that some variables are either positively or negatively correlated. 
# We can also add the quantified correlation values to the plot, change the color scheme, add a title, etc.
#Define you own panel
myPanel <- function(x, y, z, ...) {
  panel.levelplot(x,y,z,...)
  panel.text(x, y, round(z, 2))
}
#Define the color scheme
cols = colorRampPalette(c("red","blue"))
#Plot the correlation matrix.
levelplot(cor(Auto), col.regions = cols(100), main = "correlation", xlab = NULL, ylab = NULL, 
          scales = list(x = list(rot = 90)), panel = myPanel)

# Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables as the predictors.
# Build a multiple linear regression model to predict mpg (miles per gallon) by using all the other variables as predictors.
my_fit = lm(mpg~., data = Auto)
# where "mpg~." means the model regresses mpg on all the other variables

# Print the results of the model with summary() function
summary(my_fit)
# The output contains: residuals, coefficients, residual standard error,ð‘…2 and F-statistic. 
# They are the statistics that you need to assess the accuracy of your model.

# check the model results by removing insignificant variables: horsepower, acceleration
summary(lm(mpg~., data = subset(Auto, select=c( -horsepower, -acceleration ))))

# Comparing the summary results of the two models, Adjusted R-squared:  0.8182 for model1 and Adjusted R-squared:  0.8166 for model2
# F-statistic measures relationship between our predictor and the response variables.

# Use the plot() function to produce diagnostic plots of the linear regression fit.
# Further diagnose the linear model with different plots.
# The following assumptions need to be checked: constant variance, linearity and normality.

# Without specifying which plot to show, the function will give us four diagnostic plots:
par(mfcol=c(2,2))
plot(my_fit)
# The diagnostic plots show residuals in four different ways.
# The residual vs fitted plot: This plot is used to check the linear assumption.
# The normal Q-Q plot: The Q-Q plot (i.e., quantile-quantile plot) is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal.
# The scale-location plot: It is used to check the assumption of equal variance by showing if residuals are spread equally along the ranges of predictors. It is good if we can see a horizontal line with equally (randomly) spread points. The scale-location plot shows that the residuals appear randomly spread.
# The residual-leverage plot: it helps us identify influential data samples using cook distance.

# interaction effects.
# Do any interactions appear to be statistically significant? Here we would like to explore some possible interaction between variables. 
# For example, let's try to check whether or not the interaction between horsepower and weight can affect the model accuracy.
summary(update(my_fit, . ~ . + horsepower:weight))
# Compare the summary above with that generated by the model without the interaction terms:
# The model with the interaction term explains more variation of the data than the one without interaction, see the Adjusted R-squared score.


# anova() function can also be used to compare models. 
# Note that the anova() function has one strong requirement when comparing two models: 
# one model must be contained within the other. In other words, all the terms of the smaller model must appear in the larger model. 
# Otherwise, the comparison is impossible.
anova(my_fit, update(my_fit, . ~ . + horsepower:weight))
# The p-value shows that the two models are statistically different as The p-value is very low. 
# It means that adding the interaction term does make a difference. 

# remove the horsepower varibale from the model and transfer horsepower into log(horsepower) and apply to the model
summary(update(my_fit, . ~ . - horsepower + log(horsepower)))


############################################################################################################
# Analyzing sales of child car seats at 400 different stores.
Carseats = read.csv("Carseats.csv",header = T, na.strings = "?")
Carseats = na.omit(Carseats)
head(Carseats)

str(Carseats) # 400 observations and 11 variables

# change character type into categorical data type.
Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc) 
Carseats$Urban <- as.factor(Carseats$Urban) 
Carseats$US <- as.factor(Carseats$US) 


# Fit a multiple regression model to predict sales as a function of all the other variables, being careful to handle the qualitative variables.
# Build a multiple linear regression model to predict sales by using all the other variables as predictors. 
# Pay attention to is how the lm() treats categorical variables.
fit1 = lm(Sales~., data = Carseats)
summary(fit1)
# variable ShelveLoc is character type with "Bad" "Good" and "Medium" and Urban and US has value of "Yes".

# contrasts() function sets and views the contrasts associated with a factor.
contrasts(Carseats$ShelveLoc)
# 3 types of ShelveLoc: Bad, Good and Medium can be transferred into two Good and Medium to show the three type: 
# 00 - Bad, 10 - Good and 01 - Medium

# From the summury() results of the model, we can observe that these predictors, CompPrice (Price charged by competitor at each location), Income, Advertising, Price, ShelveLocation (indicating the quality of the shelving location), and Age
# are siginificant and have a strong association with the response variable Sales. 
# And other insignificant variables can be removed to make the mnodel simple.
fit2 = lm(Sales~ CompPrice + Income + Advertising + Price + ShelveLoc + Age, data = Carseats)
summary(fit2)
# Now all variables are significant.

# Evaluation the models: fit1 and fit2:
# The following items of the models can be examined:
# Look at the R-squared and F statistics and the corresponding p-values.
# Check the residuals with various residual plots.
# Perform F-tests by comparing the two models using the anova() function

# The Multiple R-squared: 0.872,Adjusted R-squared: 0.8697 in fit2 are slightly dropped comparing fit1.
# The tradoff between simple model and modeling accuracy is very small.
# Therefore, the simple model fit2 should be chose.

# Compare the diagnostic plots
par(mfcol=c(2,2))
plot(fit1, which = 1)
plot(fit1, which = 2)
plot(fit2, which = 3)
plot(fit2, which = 4)
# The plots show that both models comply with the assumptions.

# Use the anova() function:
anova(fit1, fit2)


# Select the best regression variables with the step() function:
step1 <- step(fit1)
# The last one should be the best with the biggest R-square value.

##Expand the model with interaction effects using the * and : symbols.
# Investigate how the potential interactions between predictors can effect the linear model. 
# Here we try income: advertising, price:age, or both. Do any interactions appear to be statistically significant?

# Update fit2 into a new model fit3:
fit3 = update(fit2, . ~ . + Income: Advertising)
summary(fit3)
# Income: Advertising shows as significant to the model.

fit4 = update(fit3, . ~ . + Price: Age)
summary(fit4)
# Price:Age shows as insignificant to the model.


# compare the anove() info across fit2, fit3 and fit4:
anova(fit2, fit3, fit4)
# In the ANOVA test, model fit3 is better and made a significant difference.

# Using the model fit3, obtain 95% confidence intervals for the coefficients.
confint(fit3, level = 0.95)

# There are many ways of checking if there are some outliers in the data. 
# Using residual plots to identify outliers:
plot(fit3, which = 1)
# In the plot we can find some data points that stay a bit far from the majority of the points. Can we conclude that there are some outliers?
# Use the outlierTest() function to do an outlier test. The function will report the Bonferroni p-values for studentized residuals in linear regression models.
library(car)
outlierTest(fit3, cutoff=0.05, digits = 1)
# The test shows that data point 358 is not an outlier.
# Are there any influential data points? Use the influencePlot() function. If influential points are identified, the function will also return a data frame with the hat values, Studentized residuals and Cook's distance
influencePlot(fit3, scale=5, id.method="noteworthy", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
# In the influence plot, the two dashed vertical lines are drawn at twice and three times the average hat value respectively. 
# The three horizontal lines are drawn at -2, 0 and 2 on the Studentized-residual scale.

par(mfcol=c(2,2))
plot(fit3)



